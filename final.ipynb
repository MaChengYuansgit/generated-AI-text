{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import gc\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import numpy as np\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "from tokenizers import (decoders,\n",
    "                        models,\n",
    "                        normalizers,\n",
    "                        pre_tokenizers,\n",
    "                        processors,\n",
    "                        trainers,\n",
    "                        Tokenizer)\n",
    "\n",
    "\n",
    "from datasets import Dataset\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "导入数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('/kaggle/input/llm-detect-ai-generated-text/test_essays.csv')\n",
    "sub = pd.read_csv('/kaggle/input/llm-detect-ai-generated-text/sample_submission.csv')\n",
    "org_train = pd.read_csv('/kaggle/input/llm-detect-ai-generated-text/train_essays.csv')\n",
    "train1 = pd.read_csv(\"/kaggle/input/daigt-v2-train-dataset/train_v2_drcat_02.csv\", sep=',')\n",
    "train2 = pd.read_csv('/kaggle/input/daigt-proper-train-dataset/train_drcat_04.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "org_train = org_train.rename(columns={'generated': 'label'})\n",
    "\n",
    "excluded_prompt_name_list = ['Distance learning','Grades for extracurricular activities','Summer projects']\n",
    "train1 = train1[~(train1['prompt_name'].isin(excluded_prompt_name_list))]\n",
    "\n",
    "train = pd.concat([org_train, train1, train2])\n",
    "train = pd.read_csv(\"/kaggle/input/daigt-v2-train-dataset/train_v2_drcat_02.csv\", sep=',')\n",
    "\n",
    "train.reset_index(drop=True, inplace=True)\n",
    "train = train.drop_duplicates(subset=['text'])\n",
    "train.reset_index(drop=True, inplace=True)\n",
    "\n",
    "\n",
    "train.head(2)\n",
    "train.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "unique_words = set()\n",
    "for text in train['text']:\n",
    "    words = text.lower().split()  \n",
    "    unique_words.update(words)\n",
    "\n",
    "unique_words = {word.strip(string.punctuation) for word in unique_words}\n",
    "\n",
    "\n",
    "total_unique_words = len(unique_words)\n",
    "print(\"Total unique words:\", total_unique_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for tokenization\n",
    "LOWERCASE = False\n",
    "VOCAB_SIZE = 14000000\n",
    "# VOCAB_SIZE = total_unique_words // 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Byte-Pair Encoding tokenizer\n",
    "# The [UNK] token is used to represent unknown words during tokenization.\n",
    "raw_tokenizer = Tokenizer(models.BPE(unk_token=\"[UNK]\"))\n",
    "raw_tokenizer.normalizer = normalizers.Sequence([normalizers.NFC()] + [normalizers.Lowercase()] if LOWERCASE else [])\n",
    "raw_tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel()\n",
    "\n",
    "# Specifying special tokens for the tokenizer and initializing the BPE trainer.\n",
    "# The trainer is configured with the desired vocabulary size and the special tokens.\n",
    "special_tokens = [\"[UNK]\", \"[PAD]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]\n",
    "trainer = trainers.BpeTrainer(vocab_size=VOCAB_SIZE, special_tokens=special_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the test data to a Huggingface dataset for easier handling.\n",
    "dataset = Dataset.from_pandas(test[['text']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate batches of text data for training.\n",
    "# This approach helps in managing memory usage when dealing with large datasets.\n",
    "def train_corp_iter(): \n",
    "    for i in range(0, len(dataset), 1000):\n",
    "        yield dataset[i : i + 1000][\"text\"]\n",
    "\n",
    "# Training the tokenizer on the dataset using the defined trainer.\n",
    "raw_tokenizer.train_from_iterator(train_corp_iter(), trainer=trainer)\n",
    "\n",
    "# Wrapping the trained tokenizer with Huggingface's PreTrainedTokenizerFast for additional functionalities.\n",
    "# This step integrates the tokenizer with Huggingface's ecosystem, enabling easy use with their models.\n",
    "tokenizer = PreTrainedTokenizerFast(\n",
    "    tokenizer_object=raw_tokenizer,\n",
    "    unk_token=\"[UNK]\",\n",
    "    pad_token=\"[PAD]\",\n",
    "    cls_token=\"[CLS]\",\n",
    "    sep_token=\"[SEP]\",\n",
    "    mask_token=\"[MASK]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizing the text data in the 'test' DataFrame and storing the results.\n",
    "tokenized_texts_test = []\n",
    "for text in tqdm(test['text'].tolist()):\n",
    "    tokenized_texts_test.append(tokenizer.tokenize(text))\n",
    "\n",
    "# Tokenizing the text data in the 'train' DataFrame and storing the results.\n",
    "tokenized_texts_train = []\n",
    "for text in tqdm(train['text'].tolist()):\n",
    "    tokenized_texts_train.append(tokenizer.tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the tokenized text for test data\n",
    "tokenized_texts_test[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dummy(text):\n",
    "    \"\"\"\n",
    "    A dummy function to use as tokenizer for TfidfVectorizer. It returns the text as it is since we already tokenized it.\n",
    "    \"\"\"\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize TfidfVectorizer for test set\n",
    "# Parameters: \n",
    "# - ngram_range=(3, 5): Use 3 to 5 word n-grams.\n",
    "# - lowercase=False: Maintain case sensitivity.\n",
    "# - sublinear_tf=True: Apply sublinear term frequency scaling.\n",
    "# - analyzer, tokenizer, preprocessor: Use custom 'dummy' functions.\n",
    "# - token_pattern=None: Disable default token pattern.\n",
    "# - strip_accents='unicode': Remove accents using Unicode.\n",
    "\n",
    "vectorizer = TfidfVectorizer(ngram_range=(3, 5), \n",
    "                             lowercase=False, \n",
    "                             sublinear_tf=True, \n",
    "                             analyzer = 'word',\n",
    "                             tokenizer = dummy,\n",
    "                             preprocessor = dummy,\n",
    "                             token_pattern = None,\n",
    "                             strip_accents='unicode')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit vectorizer on test data to learn vocabulary\n",
    "vectorizer.fit(tokenized_texts_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = vectorizer.vocabulary_  # Extract learned vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reinitialize TfidfVectorizer for training set using test set's vocabulary\n",
    "vectorizer = TfidfVectorizer(ngram_range=(3, 5), \n",
    "                             lowercase=False, \n",
    "                             sublinear_tf=True, \n",
    "                             vocabulary=vocab,\n",
    "                             analyzer = 'word',\n",
    "                             tokenizer = dummy,\n",
    "                             preprocessor = dummy,\n",
    "                             token_pattern = None, \n",
    "                             strip_accents='unicode')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform training and test data into TF-IDF vectors\n",
    "tf_train = vectorizer.fit_transform(tokenized_texts_train)\n",
    "tf_test = vectorizer.transform(tokenized_texts_test)\n",
    "\n",
    "# Cleanup: Free up memory\n",
    "del vectorizer\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change \"tf_test.copy()\" to \"tf_train.copy()\" to see an even clearer example of a sparse matrix.\n",
    "# Set print_bool to True to print (Make sure this is False when submitting!)\n",
    "print_bool = False\n",
    "\n",
    "if print_bool: \n",
    "    tf_demonstration_vector = tf_test.copy()\n",
    "    tf_idf_array = tf_demonstration_vector.toarray()\n",
    "\n",
    "    print(\"As can be seen, we do indeed have a sparse matrix:\")\n",
    "    print(type(tf_demonstration_vector), tf_demonstration_vector.shape)\n",
    "    print(\"\")\n",
    "    print(tf_idf_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    from catboost import CatBoostClassifier\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "#     clf2 = MultinomialNB(alpha=0.01)\n",
    "    clf = MultinomialNB(alpha=0.0225)\n",
    "#     clf2 = MultinomialNB(alpha=0.01)\n",
    "    sgd_model = SGDClassifier(max_iter=9000, tol=1e-4, loss=\"modified_huber\", random_state=6743)\n",
    "#     log_model = LogisticRegression( tol=1e-4,random_state=6743)\n",
    "    p6={'n_iter': 3000,'verbose': -1,'objective': 'cross_entropy','metric': 'auc',\n",
    "        'learning_rate': 0.00281909898961407, 'colsample_bytree': 0.78,\n",
    "        'colsample_bynode': 0.8,\n",
    "#         'lambda_l1': 4.562963348932286, \n",
    "       # 'lambda_l2': 2.97485, 'min_data_in_leaf': 115, 'max_depth': 23, 'max_bin': 898\n",
    "       }\n",
    "    p6[\"random_state\"] = 6743\n",
    "    lgb=LGBMClassifier(**p6)\n",
    "    cat=CatBoostClassifier(iterations=3000,\n",
    "                           verbose=0,\n",
    "                           random_seed=6543,\n",
    "#                            l2_leaf_reg=6.6591278779517808,\n",
    "                           learning_rate=0.002599066836106983,\n",
    "                           subsample = 0.35,\n",
    "                           allow_const_label=True,loss_function = 'CrossEntropy')\n",
    "    weights = [0.1,0.31,0.31,0.69]\n",
    "    \n",
    "    ensemble = VotingClassifier(estimators=[('mnb',clf),\n",
    "                                        ('sgd', sgd_model),\n",
    "                                        ('lgb',lgb), \n",
    "                                        ('cat', cat)\n",
    "                                        ],\n",
    "                                weights=weights, voting='soft', n_jobs=-1)\n",
    "    return ensemble\n",
    "\n",
    "model = get_model()\n",
    "print(model)\n",
    "\n",
    "if len(test.text.values) <= 5:\n",
    "    # if not, just sample submission\n",
    "    sub.to_csv('submission.csv', index=False)\n",
    "else:\n",
    "    model.fit(tf_train, y_train)\n",
    "\n",
    "#     gc.collect()\n",
    "\n",
    "    final_preds = model.predict_proba(tf_test)[:,1]\n",
    "    sub['generated'] = final_preds\n",
    "    sub.to_csv('submission.csv', index=False)\n",
    "    sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
